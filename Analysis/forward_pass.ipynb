{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bb20cb-cf4c-4925-98e9-a57d6726b70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b29548-b90e-4f32-b186-bc796eacae28",
   "metadata": {},
   "source": [
    "<p style=\"color: blue; font-size: 30px;\"><b>Forward Pass</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5fdc17-dcb0-4733-b99e-fe45c8cc54b6",
   "metadata": {},
   "source": [
    "hidden_layer_input = np.dot(X, weights_input_hidden) + bias_hidden           \n",
    "\n",
    "X :      \n",
    "[[0.5488135 0.71518937 0.60276338]]     \n",
    "\n",
    "weights_input_hidden:     \n",
    "[[0.84725174 0.6235637 ] [0.38438171 0.29753461] [0.05671298 0.27265629]] \n",
    "\n",
    "np.dot(X, weights_input_hidden) :       \n",
    "[[0.77407341 0.71936099]]      \n",
    "\n",
    "bias_hidden :     \n",
    "[[0.47997717 0.3927848 ]]    \n",
    "\n",
    "hidden_layer_input :    \n",
    "[[1.25405058 1.11214579]]      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a47ea43-a98d-4959-83bd-8d2081f5a0c0",
   "metadata": {},
   "source": [
    "# Break this down step by step Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dee6c1b-6710-4ee2-b6af-0a6bb42e34b7",
   "metadata": {},
   "source": [
    "**1. Understanding the Formula:**      \n",
    "The expression:   \n",
    "\n",
    "$$HiddenLayerInput = np.dot(X, WeightsInputHidden) + BiasHidden$$ \n",
    "\n",
    "represents a crucial step in a neural network. It calculates the input to the neurons in the hidden layer by performing:\n",
    "\n",
    "* A matrix multiplication between the **input features (X)** and **the weight matrix (weights_input_hidden)**.\n",
    "\n",
    "* Adding the **bias (bias_hidden)** to each **result for the neurons in the hidden layer**.\n",
    "\n",
    "**2. Inputs and Parameters:** \n",
    "\n",
    "* X is the input data. It's a 1x3 matrix (a single sample with 3 features): $$ X = \\begin{bmatrix} 0.5488135 & 0.71518937 & 0.60276338 \\end{bmatrix} $$\n",
    "\n",
    " * weights_input_hidden is the weight matrix for the connections from the input layer to the hidden layer. It has a shape of 3x2, meaning there are 3 input features and 2 neurons in the hidden layer: $$ WeightsInputHidden = \\begin{bmatrix} 0.84725174 & 0.6235637 \\\\ 0.38438171 & 0.29753461 \\\\ 0.05671298 & 0.27265629 \\end{bmatrix} $$\n",
    "\n",
    "* bias_hidden is the bias for the neurons in the hidden layer. It’s a 1x2 matrix (matching the number of hidden neurons): $$ BiasHidden = \\begin{bmatrix} 0.47997717 & 0.3927848 \\end{bmatrix} $$     \n",
    "\n",
    "\n",
    "**3. Matrix Multiplication (np.dot):**       \n",
    "The matrix multiplication (np.dot) computes the dot product of X and weights_input_hidden:\n",
    "\n",
    "$$ np.dot(X, WeightsInputHidden) = \\begin{bmatrix} 0.5488135 & 0.71518937 & 0.60276338 \\end{bmatrix} \\cdot \\begin{bmatrix} 0.84725174 & 0.6235637 \\\\ 0.38438171 & 0.29753461 \\\\ 0.05671298 & 0.27265629 \\end{bmatrix} $$ \n",
    "\n",
    "\n",
    "This results in: $$ np.dot(X, WeightsInputHidden) = \\begin{bmatrix} 0.77407341 & 0.71936099 \\end{bmatrix} $$\n",
    "\n",
    "Here’s how each element is calculated:\n",
    "\n",
    "First hidden neuron: $$ (0.5488135 \\cdot 0.84725174) + (0.71518937 \\cdot 0.38438171) + (0.60276338 \\cdot 0.05671298) = 0.77407341 $$\n",
    "\n",
    "Second hidden neuron: $$ (0.5488135 \\cdot 0.6235637) + (0.71518937 \\cdot 0.29753461) + (0.60276338 \\cdot 0.27265629) = 0.71936099 $$\n",
    "\n",
    "\n",
    "**4. Adding the Bias:**\n",
    "To calculate hidden_layer_input, the bias is added element-wise to the result of the dot product: $$ HiddenLayerInput = \\begin{bmatrix} 0.77407341 & 0.71936099 \\end{bmatrix}\n",
    "\n",
    "\\begin{bmatrix} 0.47997717 & 0.3927848 \\end{bmatrix} $$\n",
    "\n",
    "This gives: $$ HiddenLayerInput = \\begin{bmatrix} 1.25405058 & 1.11214579 \\end{bmatrix} $$\n",
    "\n",
    "\n",
    "**5. Interpretation:**     \n",
    "The hidden_layer_input is the combined weighted sum of the inputs and biases for each neuron in the hidden layer.     \n",
    "\n",
    "These values will typically be passed through an **activation function (like sigmoid, ReLU, etc.) to introduce non-linearity into the network**.      \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9297f3a-49e6-4e9e-9d11-e700b5a65c99",
   "metadata": {},
   "source": [
    "# Let’s calculate the dot product step by step!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0223345-ab78-4820-8195-0bec863527bd",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We are given:\n",
    "$$\\text{Vector } X = [0.5488135, 0.71518937, 0.60276338]$$\n",
    "\n",
    "$$\\text{Matrix } W (Weights Input Hidden) = \\begin{bmatrix} \n",
    "0.84725174 & 0.6235637 \\\\ \n",
    "0.38438171 & 0.29753461 \\\\ \n",
    "0.05671298 & 0.27265629 \n",
    "\\end{bmatrix}$$\n",
    "\n",
    "**Step 1: Definition**\n",
    "The dot product involves multiplying corresponding elements of a vector and matrix row-wise and summing the results.    \n",
    "\n",
    "For matrix multiplication, we calculate:\n",
    "\n",
    "$$X \\cdot W = \n",
    "\\begin{bmatrix} \n",
    "x_1 & x_2 & x_3 \n",
    "\\end{bmatrix} \\cdot \n",
    "\\begin{bmatrix} \n",
    "w_{1,1} & w_{1,2} \\\\ \n",
    "w_{2,1} & w_{2,2} \\\\ \n",
    "w_{3,1} & w_{3,2} \n",
    "\\end{bmatrix}$$\n",
    "\n",
    "**Step-by-Step Calculation:**     \n",
    "\n",
    "1. Multiply each element of the row in **X** with the corresponding elements in the columns of **weights_input_hidden**, summing up for each column.\n",
    "\n",
    "**First Column:**   \n",
    "( 0.5488135 × 0.84725174 ) + ( 0.71518937 × 0.38438171 ) + ( 0.60276338 × 0.05671298 ) \n",
    "\n",
    "Breaking it down:   \n",
    "0.5488135 × 0.84725174 = 0.464898  \n",
    "\n",
    "0.71518937 × 0.38438171 = 0.274936\n",
    "\n",
    "0.60276338 × 0.05671298 = 0.034208  \n",
    "\n",
    "Sum:\n",
    "\n",
    "0.464898 + 0.274936 + 0.034208 = 0.774042\n",
    "\n",
    "\n",
    "**Second Column:**  \n",
    "(0.5488135 × 0.6235637) + (0.71518937 × 0.29753461) + (0.60276338 × 0.27265629) \n",
    "\n",
    "Breaking it down:     \n",
    "\n",
    "0.5488135 × 0.6235637 = 0.342245\n",
    "\n",
    "0.71518937  ×0.29753461 = 0.212861  \n",
    "\n",
    "0.60276338  0.27265629 = 0.164392  \n",
    "\n",
    "Sum:\n",
    "\n",
    "0.342245+0.212861+0.164392= 0.719498\n",
    "\n",
    "**Resulting Dot Product:**     \n",
    "The result is a 1×2 matrix:       \n",
    "  \n",
    "[0.774042  0.719498]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57270b76-95c2-4c95-ac1e-b4402b7ac412",
   "metadata": {},
   "source": [
    "## Calculating  np.dot(X, weights_input_hidden) + bias_hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20600892-9c7f-4046-9c8e-6f6ffecf98c4",
   "metadata": {},
   "source": [
    "so \n",
    "results = np.dot(X, weights_input_hidden) = [0.77407341, 0.71936099]     \n",
    "\n",
    "\n",
    "bias_hidden = [0.47997717, 0.3927848 ]\n",
    "\n",
    "To add these two vectors, simply sum their respective elements:    \n",
    "\n",
    "[0.77407341 + 0.47997717 , 0.71936099 + 0.3927848]    \n",
    "\n",
    "So the result is:    \n",
    "\n",
    "[ 1.25405058 , 1.11214579 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef8bcbc-c14b-492c-b1e4-27439dc5f5e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
